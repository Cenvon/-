    1.one-hot编码：
        到目前为止，表示分类变量最常用的方法就是使用one-hot编码或N取一编码，也叫虚拟变量。
        例如   性别
                男
                女

            编码后：
               性别  男  女
               男    1   0
               女    0   1
            即可以用0-1来表示性别。这么做是为了简化分析。更专业的说法是可以避免使数据矩阵秩亏。

        在pandas数据集中有一种非常简单的方法，就是使用get_dummies函数：自动变换所有具有对
        象类型(比如字符串)的列或所有分类的列(只是pandas中的一个特殊概念)
    
        在pandas中列索引包含范围的结尾，因此'age':'1','2','12' 中是包括12这个元素的。这
        与numpy的数组切片不同，numpy不包括np.array(11)[0:10]最后一位索引编号为10的元素。

        get_dummies中columns参数显式地给出想要编码的列。(例如columns=['Int','Categorical'])

    2.分箱、离散化、线性模型与树：
        有一种方法可以让线性模型在连续上变得更加强大，就是使用特征分箱(binning，也叫离散化)将其划分为多个特征。
        两种线完全重合，说明线性回归模型和决策树做出了完全相同的预测。对于每个箱子，二者都预测一个常数值。因为
        每个箱子内的特征是不变的，所以对于一个箱子内的所有点，任何模型都会预测相同的值。比较对特征分箱前后模型
        学到的内容，我们发现，线性模型变得更加灵活了，因为现在它对每个箱子具有不同的取值。

    3.交互特征与多项式特征
        想要丰富特征表示，特别是线性模型，另一种方法是添加原始数据的交互特征和多项式特征。
        交互特征：箱子指示符与原始特征的乘积
        多项式特征：对于给定原始特征x，我们可以考虑其x^2,x^3,x^4...这在preprocessing模块的PolynomialFeature中实现。
        多项式特征在一维数据上易得到非常平滑的拟合。但高次多项式在边界上火数据很少的区域可能有极端表现。
