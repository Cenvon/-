"""
集成是合并多个机器学习模型来构造更强大模型的方法。
1.随机森林(Random Forest)
    用于解决决策树主要缺点即对训练数据过于拟合。背后的思想是每棵树的预测都相对较好，
    并且每棵树的预测都很好，但可能对部分数据过拟合。那么我们可以对这些树的结果取平
    均值来降低过拟合。既能减少过拟合又能保持树的预测能力，这可以在数学上严格证明。
    *优点：拥有决策树所有的优点，且不需要对数据进行缩放，也不需要反复调节参数就可以
    给出很好的结果。
    *缺点：对于维度非常高的稀疏矩阵(如文本数据)，随机森林的表现往往不是很好。对于这
    种数据，使用线性模型可能更适合。
    *参数：CPU核数:n_job   取-1可以调用所有的核
          随机状态:random_state  合适的值，森林中树越多对随机状态选择的鲁棒性越好
          树的数量:n_estimators  在你的时间/内存允许的情况下尽量多
          最大特征数:max_feature  决定每棵树随机性大小，一般来说默认值就可以。

2.梯度提升回归树(梯度提升机)
    虽然名字里面有回归，但是它既可以永辉回归也可以用于分类。与随机森林不同，梯度提升
    采用连续的方式构造树，每棵树都是图纠正前一棵树的错误。默认情况下，题都提升回归树
    没有随机化，而是用到了强预剪枝。梯度提升背后的思想是合并许多简单的模型，比如深度
    较小的树。每棵树只能对部分数据做出好的预测，因此，添加的树越多，可以不断迭代提高
    性能。
    *优点：与其他基于树的模型类似，同时监督学习中最强大也是最常用的模型之一。
    *缺点：需要仔细调参，从而训练时间可能会比较长，而且它也不适用高维稀疏矩阵
    *参数：树的数量:n_estimators 寻找适合的值，太大会导致模型复杂
          学习率:learning_rate  太低，需要更多的树来构造具有相似复杂度的模型
          最大树深:max_depth 一般不超过5
"""